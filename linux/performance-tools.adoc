:toc:

// 保证所有的目录层级都可以正常显示图片
:path: linux/
:imagesdir: ../image/

// 只有book调用的时候才会走到这里
ifdef::rootpath[]
:imagesdir: {rootpath}{path}{imagesdir}
endif::rootpath[]

== performance-tools

=== 常见分析方法

==== USE 方法

utilization、saturation、errors，就是对所有的资源，查看它的使用率、饱和度和错误

==== RED方法

对于每个服务，检查请求率、错误和持续时间

==== 工作负载归纳

- 负载是谁产生的，进程ID、用户ID、远端IP地址
- 负载为什么被调用，代码路径，栈追踪
- 负载的特征是什么，IOPS、吞吐量、方向类型(读取/写入)？包含变动(标准方差)。
- 负载是怎样随着时间变化的？有日常模式吗？

==== 延时分析

延时分析检查完成-项操作所用的时间,然后把时间再分成小的时间段，接着对有着最大延时的时间段再次做划分，最后定位并量化问题的根本原因°与向下钻取分析相同,延时分析也会探入软件栈的各层来找到延时问题的原因。分析可以从所施加的工作负载开始，检查工作负载是如何在应用程序中被处理的，然后深入操作系统的库、系统调用、内核以及设备驱动。

*分析示例*

- 是否存在请求延时的问题？（是的）
- 查询时间主要是on-CPU还是off-CPU等待?(off-CPU)
- 没花在CPU上的时间在等待什么？（文件系统I/O）
- 文件系统的I/O时间是花在磁盘I/O还是锁竞争上？（磁盘I/O）
- 磁盘I/O时间主要用于排队还是服务I/O(服务)
- 磁盘服务时间主要是I/O初始化还是数据传输（数据传输）

> 见性能之巅第2章


=== 常见架构

.性能调优的影响
|===
|层级 |调优对象

|应用程序 |应用程序逻辑、请求队列大小、执行的数据库请求

|数据库 |数据库表的布局、索引、缓冲

|系统调用 |内存映射或读写、同步或异步 I/O 标志

|文件系统 |记录尺寸、缓存尺寸、文件系统可调参数、日志

|存储 |RAID 级别、磁盘类型和数目、存储可调参数

|===

image::linux/image-2025-02-27-11-42-32-481.png[]


=== 性能基础

==== 性能问题快速排查


|===
|# |工具 |检查

|1
|uptime
|平均负载，可识别负载的增加或减少(比较1分钟，5分钟，15分钟的平均值)

|2
|d,esg -T \| tail
|查看包含OOM事件的内核错误

|3
|vmstat -SM 1
|系统级统计：运行队列长度、交换、CPU总体使用情况等

|4
|mpstat -P ALL 1
|CPU情况：单个CPU很繁忙，意味着西安城扩展性糟糕

|5
|pidstat 1
|每个进程的CPU使用情况：识别意外的CPU消费者，以及每个进程的用户/系统的CPU时间

|6
|iostat -szx 1
|磁盘IO：磁盘IO的瓶颈，IOPS和吞吐量，平均等待时间，忙碌百分比

|7
|free -m
|内存使用情况，包括系统的缓存

|8
|sar -n DEV 1
|网络设备I/O: 数据包和吞吐量

|9
|sar -n TCP,ETCP 1
|TCP统计：连接率、重传

|===

==== 缓存

缓存被频繁使用来提高性能。缓存是将较慢的存储层的结果存放在较快的存储层中。把磁盘的块缓存在主内存（RAM）中就是一例

`命中率 = 命中次数/(命中次数 + 失效次数)`

一般使用的都是多层缓存。CPU 通常利用多层的硬件作为主缓存（L1、L2 和L3），开始是一个非常快但是很小的缓存（L1），后续的L2 和L3 就逐渐增加了缓存容量和访问延时。这是一个在密度和延时之间经济上的权衡。

image::../image/linux/image-2025-02-27-11-47-59-514.png[]

98%和99%之间的性能差异要比10%和11%之间的性能差异大很多。由于缓存命中和失效之间的速度差异（两个存储层级），导致了这是一条非线性曲线。两个存储层级速度差异越大，曲线倾斜越陡峭。

`运行时间 =（命中率×命中延时）+（失效率×失效延`


=== 操作系统

了解操作系统和它的内核对于系统性能分析至关重要.你会经常需要进行针对系统行为的开发和测试，如系统调用是如何执行的、CPU是如何调度线程的、有限大小的内存是如何影响性能的，或者文件系统是如何处理I／O的,等等。这些行为需要你应用自
己掌握的操作系统和内核知识。

==== 内核的执行

内核是一个庞大的程序，通常有几十万行代码。内核的执行主要是按需的，例如，当用户级别的程序发起一次系统调用，或者设备发送一个中断时。一些内核线程会异步地执行一些系统维护的工作，其中可能包括内核时钟程序和内存管理任务，但是这些都是轻量级的，只占用很少的 CPU 资源。

内核是运行在特殊CPU模式下的程序，这＿特殊的CPU模式叫作内核态，在这—状态下，设备的一切访问及特权指令的执行都是被允许的。由内核来控制设备的访问，用以支持多任务处理，除非明确允许，否则进程之间和用户之间的数据是无法彼此访问的

用户程序（进程）运行在用户态下，对于内核特权操作（例如I／O）的请求是通过系统调用传递的。

内核态和用户态是在处理器上使用特权环（或保护环）实现的。

image::linux/image-2025-02-27-14-58-35-021.png[]

例如，x86处理器支持4个特权环，编号为0到3。通常只使用两个或三个:用户态、内核态和管理程序（如果存在）°访问设备的特权指令只允许在内核态下执行;在用户态下执行这些指令会触发并常,然后由内核处理

在用户态和内核态之间的切换是模式转换。

所有的系统调用都会进行模式转换。对于某些系统调用也会进行上下文切换：那些阻塞的系统调用，比如磁盘和网络 I/O，会进行上下文切换，以便在第一个线程被阻塞的时候，另一个线程可以运行。

这些模式转换和上下文切换都会增加一小部分的时间开销（CPU 周期）1，有多种优化方法来避免开销，如下所述。

- 用户态的系统调用：可以单独在用户态库中实现一些系统调用。Linux 内核通过导出一个映射到进程地址空间里的虚拟动态共享对象（vDSO）来实现，该对象包含如 `gettimeofday(2)` 和 `getcpu(2)` 的系统调用 [Drysdale 14]。
- **内存映射**：用于按需换页（见 7.2.3 节），内存映射也可以用于数据存储和其他 I/O，可避免系统调用的开销。
- **内核旁路 (kernel bypass)**：这类技术允许用户态的程序直接访问设备，绕过系统调用和典型的内核代码路径。例如，用于网络的 DPDK 数据平面开发工具包。
- **内核态的应用程序**：这些包括在内核中实现的 TUX 网络服务器 [Lever 00]，以及图 3.2 所示的 eBPF 技术。

内核态和用户态都有自己的软件执行的上下文，包括栈和注册表。一些处理器架构（例如，SPARC）为内核使用一个单独的地址空间，这意味着模式切换也必须改变虚拟内存的上下文。

==== 进程工作环境

.进程工作环境
image::linux/image-2025-02-27-15-10-10-790.png[]

.进程内存映射
image::linux/image-2025-02-27-15-12-35-529.png[]

.内核调度器
image::linux/image-2025-02-27-15-13-33-086.png[]

.虚拟文件系统
image::linux/image-2025-02-27-15-14-15-044.png[]

.I/O栈
image::linux/image-2025-02-27-15-17-15-063.png[]

image::linux/image-2025-02-27-15-23-07-550.png[]


*工具来源*

|===
|软件包 | 提供的工具

|procps | ps(1)、vmstat(8)、uptime(1)、top(1)

|util-linux | dmesg(1)、lsblk(1)、lscpu(1)

|sysstat | iostat(1)、mpstat(1)、pidstat(1)、sar(1)

|iproute2 | ip(8)、ss(8)、nstat(8)、tc(8)

|numactl | numastat(8)

|linux-tools-common linux-tools-$(uname -r) | perf(1)、turbostat(8)

|bcc-tools (aka bpfcc-tools) | opensnoop(8)、execsnoop(8)、runqlat(8)、runqlen(8)、softirqs(8)、hardirqs(8)、ext4slower(8)、ext4dist(8)、biotop(8)、biosnoop(8)、biolatency(8)、tcptop(8)、tcplife(8)、trace(8)、argdist(8)、funcount(8)、stackcount(8)、profile(8) 等

|bpfttrace | bpfttrace、basic versions of opensnoop(8)、execsnoop(8)、runqlat(8)、runqlen(8)、biosnoop(8)、biolatency(8) 等

|perf-tools-unstable | Ftrace versions of opensnoop(8)、execsnoop(8)、iolatency(8)、iosnoop(8)、bitesize(8)、funcount(8)、kprobe(8)

|trace-cmd | trace-cmd(1)

|nicstat | nicstat(1)

|ethtool | ethtool(8)

|tiptop | tiptop(1)

|msr-tools | rdmsr(8)、wrmsr(8)

|github.com/brendangregg/msr-cloud-tools | showboost(8)、cpuhot(8)、cputemp(8)

|github.com/brendangregg/pmc-cloud-tools | pmcarch(8)、cpucache(8)、icache(8)、tlbstat(8)、resstalls(8)

|===

=== `/proc`

内核统计信息的文件系统接口，`/proc`由内核动态创建，不需要任何存储设备(在内存中运行)，多数文件是只读的，为观测工具提供统计数据，一部分文件是可写的，用于控制进程和内核的行为。

*进程级别信息统计*

- **limits**: 实际的资源限制。
- **maps**: 映射的内存区域。
- **sched**: CPU 调度器的各种统计。
- **schedstat**: CPU 运行时、延时和时间分片。
- **smaps**: 映射内存区域的使用统计。
- **stat**: 进程状态和统计信息，包括总的 CPU 和内存的使用情况。
- **statm**: 以页为单位的内存使用总结。
- **status**: 标记过的 stat 和 statm 的信息。
- **fd**: 文件描述符符号链接的目录（也见 fdinfo）。
- **cgroup**: Cgroup 成员信息。
- **task**: 每个任务的统计目录。

Linux还扩展了 `/proc`，以包含系统级别统计信息，这些数据包含在这些额外的文件和目录中。

[source, bash]
----
[root@k8smaster-147 proc]# ls -Fd [a-z]*
acpi/       consoles   driver/         interrupts  key-users    loadavg  mounts@       scsi/     sys/           uptime
bootconfig  cpuinfo    dynamic_debug/  iomem       keys         locks    mtrr          self@     sysrq-trigger  version
buddyinfo   crypto     execdomains     ioports     kmsg         mdstat   net@          slabinfo  sysvipc/       vmallocinfo
bus/        devices    fb              irq/        kpagecgroup  meminfo  pagetypeinfo  softirqs  thread-self@   vmstat
cgroups     diskstats  filesystems     kallsyms    kpagecount   misc     partitions    stat      timer_list     zoneinfo
cmdline     dma        fs/             kcore       kpageflags   modules  schedstat     swaps     tty/
----

- **cpuinfo**: 物理处理器信息，包含所有虚拟 CPU、型号、时钟频率和缓存大小。
- **diskstats**: 对于所有磁盘设备的磁盘 I/O 统计。
- **interrupts**: 每个 CPU 的中断计数器。
- **loadavg**: 平均负载。
- **meminfo**: 系统内存使用明细。
- **net/dev**: 网络接口统计。
- **net/netstat**: 系统级别的网络统计。
- **net/tcp**: 活跃的 TCP 套接字信息。
- **pressure**: 压力滞留信息（PSI）文件。
- **schedstat**: 系统级别的 CPU 调度器统计。
- **self**: 为了使用方便，关联当前进程 ID 路径的符号链接。
- **slabinfo**: 内核 slab 分配器缓存统计。
- **stat**: 内核和系统资源的统计，包括 CPU、磁盘、分页、交换区、进程。
- **zoneinfo**: 内存区信息。


=== 观测工具

.观测工具
image::linux/image-2025-02-27-15-31-07-239.png[]

.静态工具分析
image::linux/image-2025-02-27-15-32-21-502.png[]

.追踪数据来源
image::linux/image-2025-02-27-15-40-08-564.png[]

==== sar 命令

sar命令提供了对内核和设备非常广泛的覆盖，甚至对风扇也能进行观测，选项 `-m` (电源管理)

.sar 命令覆盖范围
image::linux/image-2025-02-28-19-28-53-811.png[]

[source, bash]
----
# 1秒为时间间隔，采集5次TCP数据
sar -n TCP 1 5
----

==== strace

strace命令是Linux中系统调用跟踪器，跟踪系统调用，为每个系统调用打印一行摘要信息。

[source, bash]
----
# -ttt 打印第一列UNIX时间戳，单位秒，分辨率微秒
# -T 打印最后一个字段（<time>）,即系统调用持续时间，单位秒，分辨率微秒
# -p PID 跟踪的进程ID，也可指定为命令。 -f 跟踪子线程
strace -ttt -T -p 18836
# -c 选项可以对系统调用活动做一个汇总
strace -c dd if=/dev/zero of=/dev/null bs=1M count=1024
----

*strace* 开销

当前版本的strace通过linux ptrace接口采用基于断点的跟踪，这为所有系统调用的进入和返回设置了断点，这种侵入做法会使经常调用系统函数的应用程序性能下降一个数量级。

[source, bash]
----
[root@localhost ~]# dd if=/dev/zero of=/dev/null bs=1k count=5000k
5120000+0 records in
5120000+0 records out
5242880000 bytes (5.2 GB, 4.9 GiB) copied, 1.05875 s, 5.0 GB/s
[root@localhost ~]# strace -c dd if=/dev/zero of=/dev/null bs=1k count=5000k
5120000+0 records in
5120000+0 records out
5242880000 bytes (5.2 GB, 4.9 GiB) copied, 56.7417 s, 92.4 MB/s
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 50.82    4.404512           0   5120003           read
 49.18    4.262552           0   5120003           write
  0.00    0.000010           0        35        15 openat
  0.00    0.000004           0        23           close
  0.00    0.000000           0        18           fstat
  0.00    0.000000           0         1           lseek
  0.00    0.000000           0        22           mmap
  0.00    0.000000           0         3           mprotect
  0.00    0.000000           0         1           munmap
  0.00    0.000000           0         3           brk
  0.00    0.000000           0         3           rt_sigaction
  0.00    0.000000           0         4           pread64
  0.00    0.000000           0         1         1 access
  0.00    0.000000           0         2           dup2
  0.00    0.000000           0         1           execve
  0.00    0.000000           0         2         1 arch_prctl
  0.00    0.000000           0         1           futex
  0.00    0.000000           0         1           set_tid_address
  0.00    0.000000           0         1           set_robust_list
  0.00    0.000000           0         1           prlimit64
  0.00    0.000000           0         1           getrandom
  0.00    0.000000           0         1           rseq
------ ----------- ----------- --------- --------- ----------------
100.00    8.667078           0  10240131        17 total
----

==== hardware

image::linux/image-2025-03-03-22-07-33-934.png[]

- P-cache: 预取缓存（每个CPU核一个）
- W-cache: 写缓存（每个CPU核一个）
- 时钟：CPU时钟信号生成器
- 时间戳计数器：通过时钟递增，可获取高精度时间
- 微代码ROM: 快速把指令转化为电路信号
- 温度传感器：用户温度检测
- 网络接口： 如果集成在芯片里（为了高性能）

*缓存一致性*

内存可能会同时被缓存在不同的处理器的多个CPU里，当一个CPU修改了内存时，所有的缓存都需要知道他们的缓存拷贝已经失效，应该被丢弃，这样后续所有的读才会读取到新修改的拷贝，这个过程叫缓存一致性，确保了CPU永远访问正确的内存状态。

*MMU*

MMU负责虚拟地址到物理地址的转换，通过一个在芯片上集成的TLB来缓存地址转换的缓存。主存DRAM里的转换表（页表），处理缓存未命中的情况(Cache misses are satisfied by translation tables in main memory (DRAM), called page tables, which are read directly by
the MMU (hardware) and maintained by the kernel.)。

image::linux/image-2025-03-04-10-00-08-357.png[]

内核CPU调度器的主要功能：

- 分时： 可运行线程之间的多任务，优先执行优先级最高的任务
- 抢占： 一旦有高优先级线程变为可运行状态，调度器就能够抢占当前运行的线程，这样高优先级线程可以马上开始运行。
- 负载均衡：把可运行的线程移动到空闲或者不太繁忙的CPU队列中。

.内核CPU调度函数
image::linux/image-2025-03-04-11-00-39-174.png[]

> VCX: 自愿上下文切换 +
> ICX: 非自愿上下文切换 + Time sharing/preemption 分时/抢占 + Load balancing 负载均衡 + Migration 迁移 + sleep 休眠


=== *CPU的观测工具*


|===
|工具 |描述

|uptime
|平均负载

|vmstat
|包括系统级的CPU平均负载

|mpstat
|单个CPU统计信息

|sar
|历史统计信息

|ps
|进程状态

|top
|检测每个进程/线程的CPU用量

|pidstat
|每个进程/线程CPU用量分解

|time && ptime
|给一个命令计时

|turbostat
|显示CPU时钟频率和其他状态

|showboost
|显示CPU时钟频率和睿频加速

|pmcarch
|显示高级CPU周期用量

|tlbstat
|总结TLB周期

|perf
|CPU剖析和PMC分析

|profile
|CPU栈踪迹采样

|cpudist
|总结在CPU上运行的时间

|runqlat
|总计诶在CPU运行队列延时

|runqlen
|总结CPU运行队列长度

|softirqs
|总结软中断时间

|hardirqs
|总结硬中断时间

|bpftrace
|进行CPU分析的跟踪程序

|===

==== uptime

[source, bash]
----
# 查看系统负载，最后三个是1分钟、5分钟、15分钟的平均负载，通过这些值的变化就可以知道最近15分钟内系统负载的变化情况。
[root@localhost ~]# uptime
 19:38:08 up 4 days,  2:04,  4 users,  load average: 0.01, 0.00, 0.00
----

负载是以当前的资源用量（使用率）加上排队的请求（饱和度）来衡量的.想象一下一个公路收费站:你可以通过统计-天中不同时间点的负荷，计算有多少辆汽车正在被服务（使用率）以及有多少辆汽车正在排队（饱和度）。

举一个现代的例子,一个有64颗CPU的系统的平均负载为128。这意昧着平均每个CPU上有一个线程在运行’还有一个线程在等待。

==== vmstat

r列是等待的任务总数加上正在运行的任务总数。

[source, bash]
----
[root@k8smaster-ims ~]# vmstat 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 9  0      0 1961552 890952 20010716    0    0    10   187    8    9  3  2 94  0  0
----

==== mpstat

多处理器统计工具，能够报告每个CPU的统计信息

[source, bash]
----
[root@k8smaster-ims ~]# mpstat -P ALL 1
Linux 5.14.0-503.16.1.el9_5.x86_64 (k8smaster-ims)      03/04/25        _x86_64_        (32 CPU)

19:54:26     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
19:54:27     all    5.40    0.00    1.48    0.00    0.41    0.44    0.00    0.00    0.00   92.27
19:54:27       0    2.04    0.00    2.04    0.00    0.00    0.00    0.00    0.00    0.00   95.92
19:54:27       1    1.98    0.00    1.98    0.00    0.99    0.99    0.00    0.00    0.00   94.06
19:54:27       2    8.00    0.00    2.00    0.00    1.00    0.00    0.00    0.00    0.00   89.00
19:54:27       3    7.92    0.00    1.98    0.00    0.99    0.99    0.00    0.00    0.00   88.12
19:54:27       4    2.04    0.00    1.02    0.00    0.00    1.02    0.00    0.00    0.00   95.92
----

- %usr: 用户态CPU使用率，不包括%nice
- %nice: 以nice设置的优先级运行的进程的用户时间
- %sys: 系统态CPU使用率，不包括%iowait
- %iowait: 等待IO的CPU使用率
- %irq: 硬中断的CPU使用率
- %soft: 软中断的CPU使用率
- %steal: 用在服务其他租户上的时间
- %guest: 虚拟化平台虚拟CPU使用率，用在客户虚拟机上的CPU时间
- %gnice: 以nice设置的优先级运行的进程的系统时间
- %idle: 空闲CPU使用率

==== sar

系统活动报告器，可以用来观测当前活动，以及配置归档和报告历史系统信息。

- sar -q : 包括运行队列长度runq-sz(等待加上运行，与vmstat的r列相同)和平均负载值


































https://github.com/deepflowio/deepflow/blob/main/README-CN.md

https://deepflow.io/zh/ebpf-the-key-technology-to-observability/

https://cloud.tencent.com/developer/article/2310547
https://deepflow.io/docs/zh/about/overview/
https://github.com/deepflowio/deepflow/blob/main/README-CN.md


https://apache.csdn.net/66c300c1c618435984a0123b.html

https://cloud.tencent.com/developer/article/2432675
https://ost.51cto.com/posts/24940

